# -*- coding: utf-8 -*-
"""Sportify AI.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N8IfNbGFgXnTXW0wDQFHxLsrTQkJjv2F

# Fine-tuning LLMs

## Imports
"""

!pip install datasets

import math, random, torch
from textwrap import dedent
from datasets import load_dataset, Value
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display

from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline, GPT2TokenizerFast ,GPT2Config ,AutoTokenizer, AutoModelForCausalLM
import math, random, torch
from textwrap import dedent
from datasets import load_dataset ,Value
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments
from huggingface_hub import notebook_login
from openai import OpenAI
import ipywidgets as widgets
import matplotlib.pyplot as plt

"""## Load model"""

model_name = 'gpt2'
model = GPT2LMHeadModel.from_pretrained(model_name)

"""# Load tokenizer"""

tokenizer = GPT2TokenizerFast.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

"""## Using base model"""

# Device → model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device).eval()

# Prompt
recipe_prompt = "Give me recipe instructions to make a cake"

# Tokenize (gets input_ids + attention_mask)
enc = tokenizer(recipe_prompt, return_tensors="pt").to(device)

with torch.inference_mode():
    out = model.generate(
        **enc,
        max_new_tokens=180,            # how many new tokens to write
        do_sample=True,                # sampling for variety
        top_p=0.92, temperature=0.8,   # tune to taste
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

#  print only the newly generated part (strip the prompt)
gen_only = out[0, enc["input_ids"].shape[-1]:]
recipe = tokenizer.decode(gen_only, skip_special_tokens=True).strip()
print(recipe)

"""## Prompt Engineering"""

instructions = "Give me a recipe to make a chocolate cake."
context = (
    "To make a basic chocolate cake, mix 1 cup sugar, 3/4 cup flour, 1/2 cup cocoa powder, "
    "3/4 tsp baking powder, 3/4 tsp baking soda, a pinch of salt, 1 egg, 1/2 cup milk, "
    "1/4 cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30–35 minutes."
)

def build_prompt(instructions: str, context: str) -> str:
    return dedent(f"""\
    Instruction: {instructions}
    Context: {context}

    Recipe:
    1.
    """)

prompt = build_prompt(instructions, context)
print(prompt)

# Tokenize (gets input_ids + attention_mask); add truncation just in case
enc = tokenizer(prompt, return_tensors="pt", truncation=True)
enc = {k: v.to(device) for k, v in enc.items()}

model.eval()
with torch.inference_mode():
    outputs = model.generate(
        **enc,
        max_new_tokens=180,            # how many NEW tokens to write
        do_sample=True,                # optional: more variety
        top_p=0.92, temperature=0.8,   # optional: tune to taste
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

# Decode only what the model added (not the prompt)
gen_only = outputs[0, enc["input_ids"].shape[-1]:]
recipe = tokenizer.decode(gen_only, skip_special_tokens=True).strip()
print(recipe)

# Ensure pad tokens align for GPT-2
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
if model.config.pad_token_id is None:
    model.config.pad_token_id = tokenizer.eos_token_id

# Device + eval
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device).eval()

# Prompt
prompt = "Give me a recipe to make a chocolate cake"

# Tokenize (no fixed-length padding for single prompt)
enc = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)

with torch.inference_mode():
    outputs = model.generate(
        **enc,
        max_new_tokens=120,
        do_sample=True,                # set to False for deterministic output
        top_p=0.92, temperature=0.8,   # sampling controls
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

# Decode only the newly generated part
gen_only = outputs[0, enc["input_ids"].shape[-1]:]
recipe = tokenizer.decode(gen_only, skip_special_tokens=True).strip()
print(recipe)

"""## Prepare Model for Training"""

# Example tweak that’s safe for GPT-2:
model.config.pad_token_id = getattr(model.config, "pad_token_id", None) or model.config.eos_token_id

# Enable gradient checkpointing for TRAINING
if hasattr(model, "gradient_checkpointing_enable"):
    model.config.use_cache = False  # required when using grad checkpointing
    model.gradient_checkpointing_enable()

# Total number of parameters
total_params = sum(p.numel() for p in model.parameters())
print("Total parameters:", total_params)

# Total number of trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("Trainable parameters:", trainable_params)

# Percentage of trainable parameters
trainable_percentage = (trainable_params / total_params) * 100
print("Trainable percentage: {:.2f}%".format(trainable_percentage))

"""# Preparing Training Dataset"""

!pip install datasets

!pip install transformers

# Reproducibility
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)

# Load full split, shuffle, then select up to 10k examples
raw = load_dataset("darkraipro/recipe-instructions", split="train")
raw = raw.shuffle(seed=SEED)
data = raw.select(range(min(10_000, len(raw))))

# Quick sanity checks for later tokenization
print("Rows:", len(data))
print("Columns:", data.column_names)
print("Example[0]:", data[0])

# Split dataset into train and validation (e.g., 90% train, 10% validation)
split = data.train_test_split(test_size=0.1, seed=SEED)  # 10% validation, deterministic
train_dataset = split["train"]
val_dataset   = split["test"]

print(len(train_dataset), len(val_dataset))

cols = ["input_ids", "attention_mask"]
train_dataset.set_format(type="torch", columns=cols)
val_dataset.set_format(type="torch", columns=cols)

print(train_dataset)
print(val_dataset)

cols = ["input_ids", "attention_mask"]
train_dataset.set_format(type="torch", columns=cols)
val_dataset.set_format(type="torch", columns=cols)

# Sanity check
import torch
x_tr, x_val = train_dataset[0], val_dataset[0]
assert x_tr["input_ids"].dtype == torch.int64 and x_tr["attention_mask"].dtype == torch.int64
assert x_val["input_ids"].dtype == torch.int64 and x_val["attention_mask"].dtype == torch.int64

# Single example (tensor)
ex = train_dataset[0]
print(type(ex["input_ids"]), ex["input_ids"].dtype, ex["input_ids"].shape)

# Small batch (stacked tensor)
batch = train_dataset[:4]
print(type(batch["input_ids"]), batch["input_ids"].shape)

# Sanity assertions
assert ex["input_ids"].dtype == torch.int64
assert ex["attention_mask"].dtype == torch.int64

# Make sure pad/eos are aligned for GPT-2
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
if getattr(model.config, "pad_token_id", None) is None:
    model.config.pad_token_id = tokenizer.eos_token_id

# Training-friendly toggles
model.config.use_cache = False
if hasattr(model, "gradient_checkpointing_enable"):
    model.gradient_checkpointing_enable()

# Initialize data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

"""# Fine-tuning Model"""

training_args = TrainingArguments(
    output_dir="gpt2-model",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    fp16=torch.cuda.is_available(),
    logging_steps=50,
    save_steps=500,   # adjust if you like
    seed=42,
)

model.config.use_cache = False  # required during training (esp. with grad checkpointing)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

# Re-enable cache for inference and save
model.config.use_cache = True
trainer.save_model("gpt2-model")
tokenizer.save_pretrained("gpt2-model")

"""Due to the excessive computational time required, I was compelled to terminate the process.

# Push model to hub
"""

notebook_login()

hf_name = 'anf1lll'
model_id = hf_name + "/" + "gpt2-model"

print(model_id)

model.push_to_hub(model_id)

"""# Load Fine-tuned Model"""

model_id = "anf1lll/gpt2-model"

# Load tokenizer
try:
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
except Exception:
    tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)

# GPT-2 has no PAD → use EOS
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load model and align pad id
model = AutoModelForCausalLM.from_pretrained(model_id)
if getattr(model.config, "pad_token_id", None) is None:
    model.config.pad_token_id = tokenizer.eos_token_id

model.eval()

"""# Use Fine-tuned Model"""

instruction = "Can you give me the recipe to make a chocolate cake?"
comment = "To make a basic chocolate cake, mix 1 cup sugar, ¾ cup flour, ½ cup cocoa powder, ¾ tsp baking powder, ¾ tsp baking soda, a pinch of salt, 1 egg, ½ cup milk, ¼ cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30-35 minutes. Enjoy your delicious dessert!"
prompt = f"[INST] {instruction}\n{comment}\n[/INST]"
print(prompt)

# Tokenize (no fixed-length padding for a single prompt)
enc = tokenizer(prompt, return_tensors="pt", truncation=True)
enc = {k: v.to(device) for k, v in enc.items()}  # move to device

model_id = "anf1lll/gpt2-model"

try:
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
except Exception:
    tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)

# GPT-2 has no PAD -> use EOS
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Model
model = GPT2LMHeadModel.from_pretrained(model_id).to(device).eval()
if getattr(model.config, "pad_token_id", None) is None:
    model.config.pad_token_id = tokenizer.eos_token_id

# Define the prompt
instruction = "Can you give me the recipe to make a chocolate cake?"
comment = "To make a basic chocolate cake, mix 1 cup sugar, ¾ cup flour, ½ cup cocoa powder, ¾ tsp baking powder, ¾ tsp baking soda, a pinch of salt, 1 egg, ½ cup milk, ¼ cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30-35 minutes. Enjoy your delicious dessert!"
prompt = f"[INST] {instruction}\n{comment}\n[/INST]"

# Tokenize the prompt
inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)


# Generate
with torch.inference_mode():
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=50,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

# Print only what was generated (not the prompt)
gen_only = outputs[0, inputs["input_ids"].shape[-1]:]
print(tokenizer.decode(gen_only, skip_special_tokens=True).strip())

model = GPT2LMHeadModel.from_pretrained("anf1lll/gpt2-model").to(device)
model.eval()


outputs = model.generate(
input_ids=inputs["input_ids"],
attention_mask=inputs["attention_mask"],
max_new_tokens=50,
do_sample=True,
top_k=50,
top_p=0.95
)


print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# Prompt template
prompt_template = lambda instructions_string: f'''[INST] {instructions_string} \nInstructions:  \n[/INST]'''

comment = "Can you give me the recipe to make an benedict eggs?"
prompt = prompt_template(comment)

model.eval()

inputs = tokenizer(prompt, return_tensors="pt", padding="max_length", max_length=512, truncation=True).to(device)
outputs = model.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=50  # Specify how many new tokens to generate beyond the input
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

!pip install nltk

text = "Grilled chicken breast with garlic, lemon, and rosemary. Cook over medium heat for 10 minutes on each side."


tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token


def calculate_perplexity(model_name, text):
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.eval()
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        return math.exp(loss.item())


perplexity_before = calculate_perplexity("gpt2", text)
perplexity_after = calculate_perplexity("anf1lll/gpt2-model", text)


labels = ['Before Fine-Tuning', 'After Fine-Tuning']
values = [perplexity_before, perplexity_after]

plt.figure(figsize=(8, 5))
bars = plt.bar(labels, values, color=['gray', 'green'])
plt.title('Perplexity Before and After Fine-Tuning')
plt.ylabel('Perplexity')
plt.ylim(0, max(values) + 10)
plt.grid(axis='y', linestyle='--', alpha=0.7)

for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 1, f'{height:.1f}',
             ha='center', va='bottom', fontsize=12)

plt.text(0.5, max(values) - 5,
         'Lower perplexity after fine-tuning\nindicates better language generation quality',
         ha='center', va='center', fontsize=11, color='darkgreen',
         bbox=dict(facecolor='white', alpha=0.8))

plt.show()

!pip install --upgrade openai transformers ipywidgets -q

client = OpenAI(api_key="PUT HERE YOUR TOKEN")



recipe_model = GPT2LMHeadModel.from_pretrained("anf1lll/gpt2-model").to(device)
recipe_tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)
recipe_tokenizer.pad_token = recipe_tokenizer.eos_token
recipe_model.eval()


age = widgets.IntSlider(description="Age", min=10, max=100, value=25)
weight = widgets.FloatText(description="Weight (kg)", value=60)
height = widgets.FloatText(description="Height (cm)", value=160)
activity = widgets.Dropdown(options=["Low", "Moderate", "High"], description="Activity")
goal = widgets.Dropdown(options=["Weight Gain", "Weight Loss", "Maintain"], description="Goal")
button = widgets.Button(description="Generate Meal Plan")

display(age, weight, height, activity, goal, button)

output = widgets.Output()
display(output)


meal_selector = widgets.Dropdown(description="Pick a Meal")
generate_button = widgets.Button(description="Generate Recipe")
display(meal_selector, generate_button)

def on_button_clicked(b):
    with output:
        output.clear_output()

        a, w, h = age.value, weight.value, height.value
        act = activity.value
        g_goal = goal.value


        bmr = 10 * w + 6.25 * h - 5 * a + 5
        multiplier = {"Low": 1.2, "Moderate": 1.55, "High": 1.9}[act]
        calorie_target = bmr * multiplier
        if g_goal == "Weight Gain":
            calorie_target *= 1.1
        elif g_goal == "Weight Loss":
            calorie_target *= 0.8

        calorie_target = int(calorie_target)
        print(f"### Target Calories: {calorie_target} kcal/day\n")


        protein_grams = int(calorie_target * 0.3 / 4)
        fat_grams = int(calorie_target * 0.3 / 9)
        carb_grams = int(calorie_target * 0.4 / 4)

        print(f"### Detailed BMR:\n")
        print(f"- BMR: {bmr} kcal/day")
        print(f"- Protein: {protein_grams} grams")
        print(f"- Fat: {fat_grams} grams")
        print(f"- Carbs: {carb_grams} grams")

        prompt = f"""
Create a healthy daily meal plan for an adult consuming {calorie_target} calories.

Each meal must include:
- Meal name
- Key ingredients
- Approximate calories
"""

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            meal_plan = response.choices[0].message.content
            print("### Generated Meal Plan:\n")
            print(meal_plan)
        except Exception as e:
            print("OpenAI Error:", str(e))
            return

        import re
        meals = re.findall(r"Meal name:\s*(.+)", meal_plan)
        if not meals:
            meals = ["Greek yogurt with berries", "Salmon with quinoa and roasted vegetables"]

        meal_selector.options = meals

def on_generate_clicked(b):
    with output:
        print(f"\n### Generating recipe for: {meal_selector.value}\n")
        prompt = f"{meal_selector.value}\n\nInstructions:"
        inputs = recipe_tokenizer(prompt, return_tensors="pt", padding="max_length", max_length=512, truncation=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        outputs = recipe_model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=150,
            do_sample=True,
            top_k=50,
            top_p=0.95
        )
        recipe = recipe_tokenizer.decode(outputs[0], skip_special_tokens=True)
        print("### Recipe:\n")
        print(recipe if recipe.strip() else " No recipe generated. Try another meal.")

button.on_click(on_button_clicked)
generate_button.on_click(on_generate_clicked)