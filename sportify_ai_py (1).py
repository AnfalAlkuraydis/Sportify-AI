# -*- coding: utf-8 -*-
"""Sportify AI.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N8IfNbGFgXnTXW0wDQFHxLsrTQkJjv2F

# Fine-tuning LLMs

## Imports
"""



from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline, GPT2TokenizerFast ,GPT2Config ,AutoTokenizer, AutoModelForCausalLM
import math, random, torch
from textwrap import dedent
from datasets import load_dataset ,Value
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments
from huggingface_hub import notebook_login
from openai import OpenAI
import ipywidgets as widgets
import matplotlib.pyplot as plt

"""## Load model"""

model_name = 'gpt2'
model = GPT2LMHeadModel.from_pretrained(model_name)

"""# Load tokenizer"""

tokenizer = GPT2TokenizerFast.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

"""## Using base model"""

# Device → model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device).eval()

# Prompt
recipe_prompt = "Give me recipe instructions to make a cake"

# Tokenize (gets input_ids + attention_mask)
enc = tokenizer(recipe_prompt, return_tensors="pt").to(device)

with torch.inference_mode():
    out = model.generate(
        **enc,
        max_new_tokens=180,            # how many new tokens to write
        do_sample=True,                # sampling for variety
        top_p=0.92, temperature=0.8,   # tune to taste
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

#  print only the newly generated part (strip the prompt)
gen_only = out[0, enc["input_ids"].shape[-1]:]
recipe = tokenizer.decode(gen_only, skip_special_tokens=True).strip()
print(recipe)

"""## Prompt Engineering"""

instructions = "Give me a recipe to make a chocolate cake."
context = (
    "To make a basic chocolate cake, mix 1 cup sugar, 3/4 cup flour, 1/2 cup cocoa powder, "
    "3/4 tsp baking powder, 3/4 tsp baking soda, a pinch of salt, 1 egg, 1/2 cup milk, "
    "1/4 cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30–35 minutes."
)

def build_prompt(instructions: str, context: str) -> str:
    return dedent(f"""\
    Instruction: {instructions}
    Context: {context}

    Recipe:
    1.
    """)

prompt = build_prompt(instructions, context)
print(prompt)

# Tokenize (gets input_ids + attention_mask); add truncation just in case
enc = tokenizer(prompt, return_tensors="pt", truncation=True)
enc = {k: v.to(device) for k, v in enc.items()}

model.eval()
with torch.inference_mode():
    outputs = model.generate(
        **enc,
        max_new_tokens=180,            # how many NEW tokens to write
        do_sample=True,                # optional: more variety
        top_p=0.92, temperature=0.8,   # optional: tune to taste
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

# Decode only what the model added (not the prompt)
gen_only = outputs[0, enc["input_ids"].shape[-1]:]
recipe = tokenizer.decode(gen_only, skip_special_tokens=True).strip()
print(recipe)

# Ensure pad tokens align for GPT-2
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
if model.config.pad_token_id is None:
    model.config.pad_token_id = tokenizer.eos_token_id

# Device + eval
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device).eval()

# Prompt
prompt = "Give me a recipe to make a chocolate cake"

# Tokenize (no fixed-length padding for single prompt)
enc = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)

with torch.inference_mode():
    outputs = model.generate(
        **enc,
        max_new_tokens=120,
        do_sample=True,                # set to False for deterministic output
        top_p=0.92, temperature=0.8,   # sampling controls
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

# Decode only the newly generated part
gen_only = outputs[0, enc["input_ids"].shape[-1]:]
recipe = tokenizer.decode(gen_only, skip_special_tokens=True).strip()
print(recipe)

"""## Prepare Model for Training"""

# Example tweak that’s safe for GPT-2:
model.config.pad_token_id = getattr(model.config, "pad_token_id", None) or model.config.eos_token_id

# Enable gradient checkpointing for TRAINING
if hasattr(model, "gradient_checkpointing_enable"):
    model.config.use_cache = False  # required when using grad checkpointing
    model.gradient_checkpointing_enable()

# Total number of parameters
total_params = sum(p.numel() for p in model.parameters())
print("Total parameters:", total_params)

# Total number of trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("Trainable parameters:", trainable_params)

# Percentage of trainable parameters
trainable_percentage = (trainable_params / total_params) * 100
print("Trainable percentage: {:.2f}%".format(trainable_percentage))

"""# Preparing Training Dataset"""

!pip install datasets

!pip install transformers

# Reproducibility
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)

# Load full split, shuffle, then select up to 10k examples
raw = load_dataset("darkraipro/recipe-instructions", split="train")
raw = raw.shuffle(seed=SEED)
data = raw.select(range(min(10_000, len(raw))))

# Quick sanity checks for later tokenization
print("Rows:", len(data))
print("Columns:", data.column_names)
print("Example[0]:", data[0])

# Split dataset into train and validation (e.g., 90% train, 10% validation)
split = data.train_test_split(test_size=0.1, seed=SEED)  # 10% validation, deterministic
train_dataset = split["train"]
val_dataset   = split["test"]

print(len(train_dataset), len(val_dataset))

cols = ["input_ids", "attention_mask"]
train_dataset.set_format(type="torch", columns=cols)
val_dataset.set_format(type="torch", columns=cols)

print(train_dataset)
print(val_dataset)

cols = ["input_ids", "attention_mask"]
train_dataset.set_format(type="torch", columns=cols)
val_dataset.set_format(type="torch", columns=cols)

# Sanity check
import torch
x_tr, x_val = train_dataset[0], val_dataset[0]
assert x_tr["input_ids"].dtype == torch.int64 and x_tr["attention_mask"].dtype == torch.int64
assert x_val["input_ids"].dtype == torch.int64 and x_val["attention_mask"].dtype == torch.int64

# Single example (tensor)
ex = train_dataset[0]
print(type(ex["input_ids"]), ex["input_ids"].dtype, ex["input_ids"].shape)

# Small batch (stacked tensor)
batch = train_dataset[:4]
print(type(batch["input_ids"]), batch["input_ids"].shape)

# Sanity assertions
assert ex["input_ids"].dtype == torch.int64
assert ex["attention_mask"].dtype == torch.int64

# Make sure pad/eos are aligned for GPT-2
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
if getattr(model.config, "pad_token_id", None) is None:
    model.config.pad_token_id = tokenizer.eos_token_id

# Training-friendly toggles
model.config.use_cache = False
if hasattr(model, "gradient_checkpointing_enable"):
    model.gradient_checkpointing_enable()

# Initialize data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

"""# Fine-tuning Model"""

training_args = TrainingArguments(
    output_dir="gpt2-model",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    fp16=torch.cuda.is_available(),
    logging_steps=50,
    save_steps=500,   # adjust if you like
    seed=42,
)

model.config.use_cache = False  # required during training (esp. with grad checkpointing)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

# Re-enable cache for inference and save
model.config.use_cache = True
trainer.save_model("gpt2-model")
tokenizer.save_pretrained("gpt2-model")

"""Due to the excessive computational time required, I was compelled to terminate the process.

# Push model to hub
"""

notebook_login()

hf_name = 'anf1lll'
model_id = hf_name + "/" + "gpt2-model"

print(model_id)

model.push_to_hub(model_id)

"""# Load Fine-tuned Model"""

model_id = "anf1lll/gpt2-model"

# Load tokenizer
try:
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
except Exception:
    tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)

# GPT-2 has no PAD → use EOS
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load model and align pad id
model = AutoModelForCausalLM.from_pretrained(model_id)
if getattr(model.config, "pad_token_id", None) is None:
    model.config.pad_token_id = tokenizer.eos_token_id

model.eval()

"""# Use Fine-tuned Model"""

instruction = "Can you give me the recipe to make a chocolate cake?"
comment = "To make a basic chocolate cake, mix 1 cup sugar, ¾ cup flour, ½ cup cocoa powder, ¾ tsp baking powder, ¾ tsp baking soda, a pinch of salt, 1 egg, ½ cup milk, ¼ cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30-35 minutes. Enjoy your delicious dessert!"
prompt = f"[INST] {instruction}\n{comment}\n[/INST]"
print(prompt)

# Tokenize (no fixed-length padding for a single prompt)
enc = tokenizer(prompt, return_tensors="pt", truncation=True)
enc = {k: v.to(device) for k, v in enc.items()}  # move to device

model_id = "anf1lll/gpt2-model"

try:
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
except Exception:
    tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)

# GPT-2 has no PAD -> use EOS
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Model
model = GPT2LMHeadModel.from_pretrained(model_id).to(device).eval()
if getattr(model.config, "pad_token_id", None) is None:
    model.config.pad_token_id = tokenizer.eos_token_id

# Define the prompt
instruction = "Can you give me the recipe to make a chocolate cake?"
comment = "To make a basic chocolate cake, mix 1 cup sugar, ¾ cup flour, ½ cup cocoa powder, ¾ tsp baking powder, ¾ tsp baking soda, a pinch of salt, 1 egg, ½ cup milk, ¼ cup vegetable oil, and 1 tsp vanilla extract. Bake at 350°F (175°C) for 30-35 minutes. Enjoy your delicious dessert!"
prompt = f"[INST] {instruction}\n{comment}\n[/INST]"

# Tokenize the prompt
inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)


# Generate
with torch.inference_mode():
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=50,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )

# Print only what was generated (not the prompt)
gen_only = outputs[0, inputs["input_ids"].shape[-1]:]
print(tokenizer.decode(gen_only, skip_special_tokens=True).strip())

model = GPT2LMHeadModel.from_pretrained("anf1lll/gpt2-model").to(device)
model.eval()


outputs = model.generate(
input_ids=inputs["input_ids"],
attention_mask=inputs["attention_mask"],
max_new_tokens=50,
do_sample=True,
top_k=50,
top_p=0.95
)


print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# Prompt template
prompt_template = lambda instructions_string: f'''[INST] {instructions_string} \nInstructions:  \n[/INST]'''

comment = "Can you give me the recipe to make an benedict eggs?"
prompt = prompt_template(comment)

model.eval()

inputs = tokenizer(prompt, return_tensors="pt", padding="max_length", max_length=512, truncation=True).to(device)
outputs = model.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=50  # Specify how many new tokens to generate beyond the input
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))



text = "Grilled chicken breast with garlic, lemon, and rosemary. Cook over medium heat for 10 minutes on each side."


tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token


def calculate_perplexity(model_name, text):
    model = GPT2LMHeadModel.from_pretrained(model_name)
    model.eval()
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        return math.exp(loss.item())


perplexity_before = calculate_perplexity("gpt2", text)
perplexity_after = calculate_perplexity("anf1lll/gpt2-model", text)


labels = ['Before Fine-Tuning', 'After Fine-Tuning']
values = [perplexity_before, perplexity_after]

plt.figure(figsize=(8, 5))
bars = plt.bar(labels, values, color=['gray', 'green'])
plt.title('Perplexity Before and After Fine-Tuning')
plt.ylabel('Perplexity')
plt.ylim(0, max(values) + 10)
plt.grid(axis='y', linestyle='--', alpha=0.7)

for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 1, f'{height:.1f}',
             ha='center', va='bottom', fontsize=12)

plt.text(0.5, max(values) - 5,
         'Lower perplexity after fine-tuning\nindicates better language generation quality',
         ha='center', va='center', fontsize=11, color='darkgreen',
         bbox=dict(facecolor='white', alpha=0.8))

plt.show()

!pip install --upgrade openai transformers ipywidgets -q

client = OpenAI(api_key="PUT HERE YOUR TOKEN")



recipe_model = GPT2LMHeadModel.from_pretrained("anf1lll/gpt2-model").to(device)
recipe_tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True)
recipe_tokenizer.pad_token = recipe_tokenizer.eos_token
recipe_model.eval()


age = widgets.IntSlider(description="Age", min=10, max=100, value=25)
weight = widgets.FloatText(description="Weight (kg)", value=60)
height = widgets.FloatText(description="Height (cm)", value=160)
activity = widgets.Dropdown(options=["Low", "Moderate", "High"], description="Activity")
goal = widgets.Dropdown(options=["Weight Gain", "Weight Loss", "Maintain"], description="Goal")
button = widgets.Button(description="Generate Meal Plan")

display(age, weight, height, activity, goal, button)

output = widgets.Output()
display(output)


meal_selector = widgets.Dropdown(description="Pick a Meal")
generate_button = widgets.Button(description="Generate Recipe")
display(meal_selector, generate_button)

def on_button_clicked(b):
    with output:
        output.clear_output()

        a, w, h = age.value, weight.value, height.value
        act = activity.value
        g_goal = goal.value


        bmr = 10 * w + 6.25 * h - 5 * a + 5
        multiplier = {"Low": 1.2, "Moderate": 1.55, "High": 1.9}[act]
        calorie_target = bmr * multiplier
        if g_goal == "Weight Gain":
            calorie_target *= 1.1
        elif g_goal == "Weight Loss":
            calorie_target *= 0.8

        calorie_target = int(calorie_target)
        print(f"### Target Calories: {calorie_target} kcal/day\n")


        protein_grams = int(calorie_target * 0.3 / 4)
        fat_grams = int(calorie_target * 0.3 / 9)
        carb_grams = int(calorie_target * 0.4 / 4)

        print(f"### Detailed BMR:\n")
        print(f"- BMR: {bmr} kcal/day")
        print(f"- Protein: {protein_grams} grams")
        print(f"- Fat: {fat_grams} grams")
        print(f"- Carbs: {carb_grams} grams")

        prompt = f"""
Create a healthy daily meal plan for an adult consuming {calorie_target} calories.

Each meal must include:
- Meal name
- Key ingredients
- Approximate calories
"""

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            meal_plan = response.choices[0].message.content
            print("### Generated Meal Plan:\n")
            print(meal_plan)
        except Exception as e:
            print("OpenAI Error:", str(e))
            return

        import re
        meals = re.findall(r"Meal name:\s*(.+)", meal_plan)
        if not meals:
            meals = ["Greek yogurt with berries", "Salmon with quinoa and roasted vegetables"]

        meal_selector.options = meals

def on_generate_clicked(b):
    with output:
        print(f"\n### Generating recipe for: {meal_selector.value}\n")
        prompt = f"{meal_selector.value}\n\nInstructions:"
        inputs = recipe_tokenizer(prompt, return_tensors="pt", padding="max_length", max_length=512, truncation=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        outputs = recipe_model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=150,
            do_sample=True,
            top_k=50,
            top_p=0.95
        )
        recipe = recipe_tokenizer.decode(outputs[0], skip_special_tokens=True)
        print("### Recipe:\n")
        print(recipe if recipe.strip() else " No recipe generated. Try another meal.")

button.on_click(on_button_clicked)
generate_button.on_click(on_generate_clicked)
