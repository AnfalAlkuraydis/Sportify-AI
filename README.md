# ðŸ¥— Sportify AI â€” GPTâ€‘2 Recipe Generator

> **Model:** [`anf1lll/gpt2-model`](https://huggingface.co/anf1lll/gpt2-model)  
> **Data:** `darkraipro/recipe-instructions` (Hugging Face Datasets)

---

## Abstract
Sportify AI is a lightweight recipe-generation project that fineâ€‘tunes GPTâ€‘2 on public cooking instructions and pairs it with a simple mealâ€‘planning flow. A calorie estimate is used to propose a daily plan, and the fineâ€‘tuned GPTâ€‘2 produces detailed, stepâ€‘byâ€‘step recipes from the selected meal. The goal is to demonstrate an endâ€‘toâ€‘end pipelineâ€”data â†’ fineâ€‘tuning â†’ evaluation â†’ interactive useâ€”while staying minimal and easy to reproduce.

---

## Overview
- **LLM:** GPTâ€‘2 fineâ€‘tuned on cooking instructions â†’ [`anf1lll/gpt2-model`](https://huggingface.co/anf1lll/gpt2-model)  
- **Dataset:** `darkraipro/recipe-instructions` (short, imperative recipe steps)  
- **Flow:** (1) estimate calories, (2) propose a day plan, (3) generate chosen recipe with the fineâ€‘tuned model  
- **Evaluation:** quick perplexity comparison (base vs. fineâ€‘tuned)  
- **Use cases:** teaching, demos, and fast experimentation with recipe generation

---

## Pipeline
1. **Prepare data** â†’ load `darkraipro/recipe-instructions` with ðŸ¤— Datasets; filter/clean short imperative steps.  
2. **Tokenize** â†’ GPTâ€‘2 tokenizer with `pad_token = eos_token`.  
3. **Fineâ€‘tune** â†’ Causal LM training (`mlm=False`) using ðŸ¤— `Trainer`.  
4. **Evaluate** â†’ compute perplexity on a heldâ€‘out split; compare to base GPTâ€‘2.  
5. **Serve/Use** â†’ load [`anf1lll/gpt2-model`](https://huggingface.co/anf1lll/gpt2-model) for recipe generation in a tiny UI or script.

---

## Results
- **Quality signal:** Perplexity drops on recipe text after fineâ€‘tuning (lower is better).  
- **Observed behavior:** More coherent, imperative, and onâ€‘topic instructions vs. base GPTâ€‘2.  

> Reproduce your own numbers with the snippet in **Reproduce Results** below.

---

## Installation
Python 3.9+ is recommended. For GPU training, install a CUDA build of PyTorch.

```bash
pip install -U torch transformers datasets accelerate huggingface_hub ipywidgets matplotlib
```

If using Jupyter:
```bash
jupyter nbextension enable --py widgetsnbextension
```

## License
Add your repository license (e.g., MIT). Respect the dataset/model licenses and API terms.
