# ü•ó Sportify AI ‚Äî Recipe Planner & GPT‚Äë2 Fine‚ÄëTuning Demo

Sportify AI is a hands‚Äëon notebook/script that **fine‚Äëtunes GPT‚Äë2 on cooking instructions** and combines it with a small **meal‚Äëplanning UI**. The UI estimates daily calories from basic inputs (age, weight, height, activity, goal), asks an LLM to suggest a menu, and then uses your **fine‚Äëtuned GPT‚Äë2** to generate step‚Äëby‚Äëstep recipes for the selected meal.

> Built with: PyTorch ‚Ä¢ ü§ó Transformers/Datasets ‚Ä¢ ipywidgets ‚Ä¢ Matplotlib ‚Ä¢ OpenAI API

---

## ‚ú® Features

- **Quick inference with base GPT‚Äë2** to generate recipes from prompts.
- **Fine‚Äëtuning GPT‚Äë2** on the public `darkraipro/recipe-instructions` dataset.
- **Perplexity check & bar chart** comparing base vs. fine‚Äëtuned model.
- **Lightweight UI (ipywidgets)** to calculate calories and generate a **daily meal plan**.
- **Two‚Äëstage recipe flow**: plan with Chat Completions ‚Üí detailed recipe with your **fine‚Äëtuned GPT‚Äë2**.
- **Hugging Face Hub integration** to push and later reload the fine‚Äëtuned model.

---

## üì¶ Requirements

> Python 3.9+ and a CUDA‚Äëenabled GPU are recommended for training.

Install the core dependencies:

```bash
pip install -U torch transformers datasets accelerate huggingface_hub openai ipywidgets matplotlib
```

If you run in Jupyter, enable widgets (once per environment):

```bash
jupyter nbextension enable --py widgetsnbextension
```

> The project is notebook‚Äëstyle; running it in **Jupyter/Colab** is the smoothest experience.

---

## üîê API Keys (OpenAI)

The UI‚Äôs meal plan step calls the OpenAI Chat Completions API. Set your key via an environment variable and initialize the client from that variable instead of hard‚Äëcoding it:

```python
import os
from openai import OpenAI
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
```

**Never commit your key** to version control.

---

## üóÇÔ∏è Project Layout (minimal)

```
.
‚îú‚îÄ‚îÄ sportify_ai.py           # The notebook-style script (originally from Colab)
‚îî‚îÄ‚îÄ README.md
```

> Your uploaded file may be named slightly differently (e.g., `Sportify AI.py`).

---

## üöÄ Quick Start (Inference Only)

Generate a simple recipe with base GPT‚Äë2:

```python
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tok = GPT2TokenizerFast.from_pretrained("gpt2")
tok.pad_token = tok.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2").to(device).eval()

prompt = "Give me recipe instructions to make a cake"
enc = tok(prompt, return_tensors="pt").to(device)

with torch.inference_mode():
    out = model.generate(**enc, max_new_tokens=180, do_sample=True, top_p=0.92, temperature=0.8,
                         eos_token_id=tok.eos_token_id, pad_token_id=tok.eos_token_id)

gen_only = out[0, enc["input_ids"].shape[-1]:]
print(tok.decode(gen_only, skip_special_tokens=True).strip())
```

---

## üß™ Fine‚ÄëTuning Guide

This project demonstrates fine‚Äëtuning GPT‚Äë2 on **recipe instructions**:

1. **Dataset**
   - `darkraipro/recipe-instructions` via ü§ó Datasets.

2. **Tokenizer & Model**
   - GPT‚Äë2 with `pad_token = eos_token` and aligned `pad_token_id` on the model config.

3. **Trainer Setup**
   - `TrainingArguments` with gradient accumulation and optional fp16 on CUDA.
   - `DataCollatorForLanguageModeling(mlm=False)` for causal LM.

4. **Train**
   - `num_train_epochs=5`, `per_device_train_batch_size=4`, `gradient_accumulation_steps=8` (tune as needed).

5. **Save & (Optionally) Push**
   - Save to `gpt2-model/` and optionally `push_to_hub()` after `huggingface_hub` login.

> **Tip:** If you see OOM (out‚Äëof‚Äëmemory), reduce `per_device_train_batch_size`, shorten `max_length`, or increase `gradient_accumulation_steps`.

---

## üìä Perplexity Check

The script computes and plots perplexity **before** vs **after** fine‚Äëtuning to give a quick quality signal. Lower is better.

---

## üß© The Meal‚ÄëPlan ‚Üí Recipe Flow

1. **Enter inputs**: age, weight (kg), height (cm), activity (Low/Moderate/High), goal (Gain/Loss/Maintain).
2. **Click ‚ÄúGenerate Meal Plan‚Äù**: uses the OpenAI Chat Completions API to propose a daily plan.
3. **Pick a meal** from the dropdown and **Generate Recipe**: your **fine‚Äëtuned GPT‚Äë2** expands it into detailed instructions.

> If Chat Completions returns an unexpected format, the script falls back to a couple of default meal names.

---

## ‚òÅÔ∏è Push & Reload from Hugging Face

```python
from huggingface_hub import notebook_login
notebook_login()  # one-time
model.push_to_hub("YOUR_USERNAME/gpt2-model")
```

Then load it anywhere:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tok = AutoTokenizer.from_pretrained("YOUR_USERNAME/gpt2-model", use_fast=True)
model = AutoModelForCausalLM.from_pretrained("YOUR_USERNAME/gpt2-model")
```

---

## üõ†Ô∏è Troubleshooting

- **`pad_token` issues**: Always set `tokenizer.pad_token = tokenizer.eos_token` for GPT‚Äë2 and align `model.config.pad_token_id`.
- **Long training time**: Use a smaller subset, fewer epochs, or a smaller model (e.g., `distilgpt2`).
- **Rate limits / API errors**: Catch and print exceptions around the OpenAI call; ensure the `OPENAI_API_KEY` is set.
- **Widgets not visible**: Ensure Jupyter widgets are enabled and you‚Äôre in a notebook context.

---

## ‚úÖ License & Attribution

- Dataset: `darkraipro/recipe-instructions` (see dataset license on Hugging Face).
- This project uses OpenAI and ü§ó Transformers/Datasets under their respective licenses.
- Add your preferred project license (e.g., MIT) at the repository root.

---

## üôå Acknowledgements

- ü§ó Hugging Face (Transformers, Datasets, Hub)
- OpenAI (Chat Completions API)
- PyTorch
- The dataset authors/maintainers

---

### Why this project?

It‚Äôs a compact, end‚Äëto‚Äëend example that blends **LLM fine‚Äëtuning** with a **tiny, practical UI**‚Äîgreat for demos, teaching, and experimenting with recipe‚Äëgeneration workflows.
